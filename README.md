# BabyBLUE-llm Benchmark

This repository contains the implementation of the BabyBLUE-llm benchmark, designed to evaluate large language models on their susceptibility to hallucinations and jailbreak attempts. Below are detailed instructions for generating and evaluating completions using this benchmark.

## Evaluate Completions

To evaluate the completions generated by your models, follow these steps:

1. **Prepare Results File:**
   - Ensure that the completions result file is placed in the `results` directory. The file should contain the output from the models you wish to evaluate.

2. **Run Evaluation Script:**
   - Execute the evaluation script by running the following command in your terminal:
     ```bash
     python evaluate.py
     ```
   - This script will process the results file and provide an evaluation based on the BabyBLUE-llm benchmark metrics.

## Generate Completions

To generate completions using the benchmark, follow these steps:

1. **Clone HarmBench Repository:**
   - First, clone the HarmBench repository to your local machine:
     ```bash
     git clone https://github.com/centerforaisafety/HarmBench
     ```

2. **Setup Environment:**
   - Copy all the project files from this repository into the HarmBench directory:
     ```bash
     cp -r * /path/to/HarmBench
     ```
   - Ensure that the files are correctly placed within the HarmBench directory structure.

3. **Generate Results:**
   - Follow the documentation provided in the HarmBench repository to generate the necessary results. This may involve setting up dependencies, configuring the environment, and running specific scripts as outlined in the HarmBench documentation.

4. **Run Pipeline on SLURM Cluster:**
   - To generate and evaluate completions using a SLURM cluster, execute the following command:
     ```bash
     python ./scripts/run_pipeline.py --methods ZeroShot,PEZ,TAP --models baichuan2_7b,mistral_7b,llama2_70b --step 2_and_3 --mode slurm
     ```
   - This command will run the specified methods and models, generating completions and evaluating them according to the BabyBLUE-llm benchmark.

## Acknowledgements

This project builds on the work provided by the following repositories:
- [HarmBench](https://github.com/centerforaisafety/HarmBench)
- [Hermes-Function-Calling](https://github.com/NousResearch/Hermes-Function-Calling)

Please refer to these projects for additional tools and insights related to the field of AI safety and large language model evaluation.

## Citation

If you use this benchmark in your research, please cite our paper:

```bibtex
@misc{mei2024not,
      title={"Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak}, 
      author={Lingrui Mei and Shenghua Liu and Yiwei Wang and Baolong Bi and Jiayi Mao and Xueqi Cheng},
      year={2024},
      eprint={2406.11668},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
